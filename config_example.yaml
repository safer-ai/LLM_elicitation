# --- API Keys ---
# Provide the API key for the provider specified in llm_settings.
# Leave the other key blank or remove the line if not used.
# IMPORTANT: Do NOT commit this file with keys filled in if using version control.
anthropic_api_key: "ANTHROPIC_API_KEY_HERE"   # Your Anthropic API key. Required if the model is from Anthropic
openai_api_key: "YOUR_OPENAI_API_KEY_HERE"     # Your OpenAI API key. Required if the model is from OpenAI.

# --- Input Data Paths ---
# Paths should be relative to the project root directory (where the main script is run)
# or absolute paths.
prompts_dir: input_data/prompts               # Directory containing prompt template (.txt) files used in the analysis.
expert_profiles_file: input_data/expert_profiles.yaml # YAML file defining the simulated expert personas.
default_benchmark_file: input_data/benchmark/bountybench_ordered.yaml # Default benchmark file. Specific steps in scenario_file can override this.
scenario_file: input_data/scenario/OC3_Ransomware_large.yaml #File for a specific scenario. Each scenario has a general description, an attacker, a target and steps.


# --- LLM & API Interaction Settings ---
# Controls the behavior of the LLM and how the script interacts with the API.
llm_settings:
  model: "claude-sonnet-4-5"          # The specific model identifier string for the chosen api_provider. E.g., "claude-3-5-sonnet-20240620", "gpt-4o".
  temperature: 1                      # Controls the randomness of the LLM's output. Higher values (e.g., 1.0) make output more random, lower values (e.g., 0.2) make it more deterministic.
  max_concurrent_calls: 10               # Maximum number of API calls allowed to be in flight simultaneously. Helps manage load and prevent overwhelming the API.
  rate_limit_calls: 45                  # Max number of API calls allowed within the 'rate_limit_period'. Set this based on your API plan's Requests Per Minute (RPM) limit to avoid errors.
  rate_limit_period: 60                 # The duration (in seconds) of the sliding time window used to enforce 'rate_limit_calls'. Typically set to 60 to match RPM limits.
  thinking:
    enabled: True
    budget_tokens: 6000
# --- Delphi Workflow Settings ---
# Controls the execution parameters of the Delphi estimation process.
workflow_settings:
  num_tasks: 4                         # Maximum number of tasks to process *from each step's benchmark*. Set to null to process all tasks. Uses the first N tasks from that benchmark.
  scenario_steps: null                  # List of specific step names from the scenario file to run probability estimation on. Set to null to process all steps, or an empty list [] to skip probability estimation. 
  num_experts: 5                        # Maximum number of expert personas to simulate from the expert_profiles_file. Set to null or remove to use all experts. Uses the first N experts.
  delphi_rounds: 1                      # The total number of iterative refinement rounds to perform. A value of 1 means only the initial estimation is done, with no refinement.
  convergence_threshold: 0.05           # Threshold for standard deviation of expert estimates in a round. If std dev falls below this, the Delphi process for that task stops early. Set high (e.g., 1.0 or 100.0) to effectively disable early stopping and always run all delphi_rounds.

  # --- Scenario-Level Metric Estimations (per benchmark task) ---
  # Set these to true to enable estimation of these additional metrics alongside probability.
  # These estimations are conditioned on the LLM's capability level (indicated by the benchmark task)
  # and use the full scenario as context.
  estimate_num_actors_per_task_benchmark: true  # Default to false
  estimate_num_attacks_per_task_benchmark: true # Default to false
  estimate_damage_per_task_benchmark: true      # Default to false

  # Include examples of easier tasks from the benchmark in the prompt to help guide the LLM.
  include_easier_tasks: true
  num_example_tasks: 2